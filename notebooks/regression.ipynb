{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import datetime\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "#'database_pipeline'\n",
    "\n",
    "class MongoDB():\n",
    "    def __init__(self,dbname):\n",
    "        self.client = MongoClient()\n",
    "        self.db = self.client[dbname]\n",
    "\n",
    "    def get_collection_name(self):\n",
    "        #Return all collection names\n",
    "        return self.db.list_collection_names()\n",
    "\n",
    "    def find(self, collection, what=dict(), _id=False, last=False):\n",
    "        #by default the function doesn't select the _id field\n",
    "        #retourne une list de dictionaire last=False\n",
    "        #retourne un dictionaire si last=True\n",
    "        if _id == False:\n",
    "            cursor = self.db[collection].find(what, {'_id': False})\n",
    "        else:\n",
    "            cursor = self.db[collection].find(what)\n",
    "\n",
    "        if last == True:\n",
    "            cursor = cursor.sort([(\"Time\", -1)]).limit(1)\n",
    "            return cursor[0]\n",
    "\n",
    "        return self.cursor_to_dict(cursor)\n",
    "\n",
    "    def find_all_last(self, what=dict(), _id=False, last=False):\n",
    "        d = dict()\n",
    "        for coll in self.get_collection_name():\n",
    "            if coll == \"users\":\n",
    "                continue\n",
    "            d[coll] = self.find(coll, what, _id, last)\n",
    "        return d\n",
    "\n",
    "    def insert_one(self, collection, item):\n",
    "        try:\n",
    "            #Insert l'item dans la base de données\n",
    "            self.db[collection].insert_one(item)\n",
    "            return 1\n",
    "        except:\n",
    "            print(\"Item non importé\")\n",
    "            return -1\n",
    "        \n",
    "    def insert_many(self, collection, liste):\n",
    "        try:\n",
    "            #Insert touts les items de la liste dans la base de données\n",
    "            #En une seule commande\n",
    "            self.db[collection].insert_many(liste)\n",
    "            return 1\n",
    "        except:\n",
    "            print(\"Liste d'item non importé\")\n",
    "            return -1\n",
    "\n",
    "    def cursor_to_dict(self, cursor):\n",
    "        l = list()\n",
    "        for i in cursor:\n",
    "            l.append(i)\n",
    "        return l\n",
    "\n",
    "    def get_keys(self, collection):\n",
    "        #Return all field name of a collection\n",
    "        map = Code(\"function() { for (var key in this) { emit(key, null); } }\")\n",
    "        reduce = Code(\"function(key, stuff) { return null; }\")\n",
    "        result = self.db[collection].map_reduce(map, reduce, \"myresults\")\n",
    "        return result.distinct('_id')\n",
    "\n",
    "#docs = list(self.db[collec_pipe_name].find().sort([('Time', -1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        Ouvre et lis le fichier passer en parmètre, reconnais seulement les types de\n",
    "        fichier supportés par pandas : CSV, JSON, HTML, Local clipboard, MS Excel,\n",
    "        HDF5 Format, feather Format, Parquet Format, Msgpack, Stata, SAS, Python Pickle\n",
    "        Format, SQL, Google Big Query. Retourne les données lu.\n",
    "\n",
    "        :params:\n",
    "            path: path of the file\n",
    "\n",
    "        :type params:\n",
    "            path: string\n",
    "\n",
    "        :return: object containing the data loaded in memory, or return -1 if type\n",
    "                 not recognize.\n",
    "    '''\n",
    "    #On récupère le nom de l'extension du fichier\n",
    "    type = path.split(\".\")[-1]\n",
    "    #Selection de la bonne fonction de pandas à utiliser\n",
    "    func_to_call = 'read_{}'.format(type)\n",
    "\n",
    "    #Récupération de l'attribut de la fonction pour l'appeler\n",
    "    try :\n",
    "        func = getattr(pd, func_to_call)\n",
    "    except :\n",
    "        print(\"Pas de fonction disponible dans pandas pour lire les données\")\n",
    "        return -1\n",
    "\n",
    "    #Lecture des données\n",
    "    try :\n",
    "        return func(path)\n",
    "    except :\n",
    "        print(\"Incorrect path\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descript_df(dataframe):\n",
    "    '''\n",
    "        Permet de faire une description rapide du dataframe de sortie. Retourne une\n",
    "        description rapide des données.\n",
    "\n",
    "        :params:\n",
    "            dataframe: de faire une description rapide du dataframe de sortie\n",
    "\n",
    "        :type params:\n",
    "            dataframe: pandas.dataframe\n",
    "\n",
    "        :return: description rapide des données\n",
    "        :rtype: string\n",
    "    '''\n",
    "    print('Matrice de corrélation : \\n')\n",
    "    corr = dataframe.corr()\n",
    "    corr_color = plt.matshow(corr, cmap=plt.cm.Reds)\n",
    "\n",
    "    # Pour chaque colonne, montrer la répartition des valeurs (vérifier les valeurs aberrantes)\n",
    "    # Kde et Histogramme\n",
    "    i = 1\n",
    "    for column in dataframe:\n",
    "        i += 1\n",
    "        plt.figure(i, figsize=(15,3))\n",
    "        plt.subplot(121)\n",
    "        dataframe[column].plot.kde()\n",
    "        plt.title('Répartition de ' + column + ' : ')\n",
    "        plt.subplot(122)\n",
    "        dataframe[column].hist()\n",
    "        plt.title('Histogramme de ' + column + ' : ')\n",
    "\n",
    "    display = corr\n",
    "    return display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def bootstrap(pipeline, data, features, target, n):\n",
    "    '''\n",
    "        Effectue un boostrap de la pipeline sur les données passées en paramètre.\n",
    "        \n",
    "        :params:\n",
    "            pipeline : pipeline\n",
    "            data : dataframe\n",
    "            features : colonnent à utiliser pour la régréssion\n",
    "            target : colonne à prédire\n",
    "            n : nombres boostrap à faire\n",
    "\n",
    "        :return:\n",
    "            pred : dictionnaire contenant les valeurs bootstrap sous forme de liste pour chaque echantillons.\n",
    "    '''\n",
    "    length = len(data)\n",
    "    keys = range(length)\n",
    "    pred = {key: list() for key in keys}\n",
    "\n",
    "    for b in range(n):\n",
    "        #Random choice\n",
    "        np.random.seed(b)\n",
    "        index = np.random.choice(range(length), int(length/0.7))\n",
    "        index_test = data.index.difference(index)\n",
    "        train = data.loc[index]\n",
    "        test = data.loc[index_test]\n",
    "\n",
    "        #Fit des données\n",
    "        pipeline.fit(train[features], train[target])\n",
    "\n",
    "        #Prédiction et score sur la base de test\n",
    "        for i, p in zip(index_test, pipeline.predict(test[features])):\n",
    "            pred[i].append(p[0])\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def compute_regression(pipeline, df, features, target, n):\n",
    "    '''\n",
    "        Permet d'obtenir les résultats des indicateurs de performances d'une régression par crossvalidation, \n",
    "        et retourne ces derniers.\n",
    "\n",
    "        :params:\n",
    "            pipeline : object de type pipeline\n",
    "            df : tuple des bases d'apprentissage et de test\n",
    "            features : colonnent à utiliser pour la régréssion\n",
    "            target : colonne à prédire\n",
    "            n : nombre de bootstrap pour le calcul des intervalles de confiance\n",
    "            BDD : booléen, pour True les résultats sont stockés dans la BDD pour\n",
    "                  False ils ne sont pas sauvegardés\n",
    "        :return:\n",
    "            r2 : score R2\n",
    "            variance : variance gloable expliquée par le modèle\n",
    "            intervalle_10 : une liste de 10 intervalles de confiances pris dans la list triés\n",
    "                            des intervalles de confiances.\n",
    "            intervalle_mean : moyenne de tous les intervalles de confiance\n",
    "    '''\n",
    "    \n",
    "    # Calcul interval de confiance par bootstrap\n",
    "    pred = bootstrap(pipe_test, df, features, target, 200)\n",
    "    inter_every_x = [np.mean(pred[i]) - 2 * np.std(pred[i]) for i in pred.keys()]\n",
    "    step = int(len(df)/10)\n",
    "    intervalle_10 = [inter_every_x[i] for i in [step, step*2, step*3, step*4, step*5, step*6,\n",
    "                                            step*7, step*8, step*9, step*10]]\n",
    "    mean_inter = np.mean(inter_every_x)\n",
    "\n",
    "    # Calcul de la variance globale expliqué et r2 par Cross-Validation\n",
    "    scoring = {\"variance\" : \"explained_variance\",\n",
    "          \"r2\" : \"r2\"}\n",
    "    result = cross_validate(pipeline, data[features], data[target], cv=7, scoring=scoring)\n",
    "    r2 = mean(result[\"test_r2\"])\n",
    "    variance = np.mean(result[\"test_variance\"])\n",
    "    \n",
    "    result = {\"r2\": r2, \"variance\": variance, \"intervalle_10\" : intervalle_10, \"intervalle_mean\" : mean_inter}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(pipeline, modele, df,  features, target, n, BDD=True):\n",
    "    '''\n",
    "        Permet d'appeler les bons indicateurs de performances et de récupérer\n",
    "        les informations utiles selon la pipeline passé en paramètre. Retourne les\n",
    "        performances associées.\n",
    "\n",
    "        :params:\n",
    "            pipeline : object de type pipeline\n",
    "            modele : type du modèle utilisé : régression/classification)\n",
    "            features : colonnent à utiliser pour la régréssion\n",
    "            target : colonne à prédire\n",
    "            n : nombre de bootstrap pour le calcul des intervalles de confiance\n",
    "            BDD : booléen, pour True les résultats sont stockés dans la BDD pour\n",
    "                  False ils ne sont pas sauvegardés\n",
    "\n",
    "        :type params:\n",
    "            bool_type_modele: boolean\n",
    "            base: tuple\n",
    "\n",
    "        :return: les performances des différents indicateurs et graphiques\n",
    "    '''\n",
    "    if (modele == \"regression\") :\n",
    "        print(\"Choix du type d'estimateur : Régression \\n\")       \n",
    "        result = compute_regression(pipeline, df, features, target, n)\n",
    "\n",
    "    elif(modele == \"classification\"):\n",
    "        print(\"Choix du type d'estimateur : Classification \\n\")\n",
    "        result = compute_classification(pipeline, df)\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    if BDD == True:\n",
    "        result['Time'] = datetime.datetime.now()\n",
    "        result[\"_id\"] = pipeline.name + \".\" + str(result['Time'])\n",
    "        mongo = MongoDB(\"database_pipeline\")\n",
    "        mongo.insert_one(pipeline.name, result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(pipe_name):\n",
    "    '''\n",
    "        Selectionne et renvoie la pipeline selectionnée\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-348d06928e8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Pipeline à utiliser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m data = pd.read_csv(\"./../data/headbrain.csv\", skiprows=1, \n\u001b[0m\u001b[0;32m      4\u001b[0m                       names=['gender','age_range' , 'head_size', 'brain_weight'])\n\u001b[0;32m      5\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Pipeline à utiliser\n",
    "\n",
    "data = pd.read_csv(\"./../data/headbrain.csv\", skiprows=1, \n",
    "                      names=['gender','age_range' , 'head_size', 'brain_weight'])\n",
    "Y = data.iloc[:,0:3]\n",
    "\n",
    "def add_metadata_property(obj, name):\n",
    "    '''\n",
    "        Permet d'ajouter un atribut à un objet existant\n",
    "    '''\n",
    "    setattr(obj, \"name\", name)\n",
    "\n",
    "# Création de la pipeline\n",
    "pipe_elias = Pipeline([\n",
    "    ('features', StandardScaler()),\n",
    "    ('estimator', neighbors.KNeighborsRegressor())   \n",
    "])\n",
    "\n",
    "# Ajout d'un nom à la pipeline\n",
    "add_metadata_property(pipe_elias, 'pipe_elias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_performance(pipe_elias, \"regression\", data, [\"age_range\", \"head_size\"], [\"brain_weight\"], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
