{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    '''\n",
    "        Permet d'obtenir un visuel rapide et clair des données. Retourne un dataframe\n",
    "        contenant les données.\n",
    "        \n",
    "        :params:\n",
    "            data_name: nom du fichier où se trouvent les données\n",
    "        \n",
    "        :type params:\n",
    "            data_name: string\n",
    "        \n",
    "        :return: dataframe contenant les données\n",
    "        :rtype: pandas.dataframe\n",
    "    '''\n",
    "    _, ext = os.path.splitext(data_name)\n",
    "    \n",
    "    if ext == '.csv' : \n",
    "        with open(data_name, newline='') as csvfile:\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline(), [',',';'])\n",
    "            csvfile.close()\n",
    "        df = pd.read_csv(data_name, dialect=dialect)\n",
    "    \n",
    "    if ext == '.json' :\n",
    "        print('json')\n",
    "        \n",
    "    print(descript_df(df))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def load_all_data(path):\n",
    "    '''\n",
    "        Ouvre et lis le fichier passer en parmètre, reconnais seulement les types de\n",
    "        fichier supportés par pandas : CSV, JSON, HTML, Local clipboard, MS Excel,\n",
    "        HDF5 Format, feather Format, Parquet Format, Msgpack, Stata, SAS, Python Pickle\n",
    "        Format, SQL, Google Big Query. Retourne les données lu.\n",
    "        \n",
    "        :params:\n",
    "            path: path of the file\n",
    "        \n",
    "        :type params:\n",
    "            path: string\n",
    "        \n",
    "        :return: object containing the data loaded in memory, or return -1 if type\n",
    "                 not recognize.\n",
    "    '''\n",
    "    #On récupère le nom de l'extension du fichier\n",
    "    type = path.split(\".\")[-1]\n",
    "    #Selection de la bonne fonction de pandas à utiliser\n",
    "    func_to_call = 'read_{}'.format(type)\n",
    "    #Récupération de l'attribut de la fonction pour l'appeler\n",
    "    try :\n",
    "        func = getattr(pd, func_to_call)\n",
    "    except :\n",
    "        print(\"Pas de fonction disponible dans pandas pour lire les données\")\n",
    "        return -1\n",
    "    #Lecture des données\n",
    "    try :\n",
    "        return func(path)\n",
    "    except :\n",
    "        print(\"path incorrect\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descript_df(dataframe):\n",
    "    '''\n",
    "        Permet de faire une description rapide du dataframe de sortie. Retourne une\n",
    "        description rapide des données.\n",
    "        \n",
    "        :params:\n",
    "            dataframe: de faire une description rapide du dataframe de sortie\n",
    "        \n",
    "        :type params:\n",
    "            dataframe: pandas.dataframe\n",
    "        \n",
    "        :return: description rapide des données\n",
    "        :rtype: string\n",
    "    '''   \n",
    "    print('Matrice de corrélation : \\n')\n",
    "    corr = dataframe.corr()\n",
    "    corr_color = plt.matshow(corr, cmap=plt.cm.Reds)\n",
    "    \n",
    "    # Pour chaque colonne, montrer la répartition des valeurs (vérifier les valeurs aberrantes)\n",
    "    # Kde et Histogramme\n",
    "    i = 1\n",
    "    for column in dataframe:\n",
    "        i += 1\n",
    "        plt.figure(i, figsize=(15,3))\n",
    "        plt.subplot(121)\n",
    "        dataframe[column].plot.kde()\n",
    "        plt.title('Répartition de ' + column + ' : ')\n",
    "        plt.subplot(122)\n",
    "        dataframe[column].hist()\n",
    "        plt.title('Histogramme de ' + column + ' : ')\n",
    "    \n",
    "    display = corr\n",
    "    return display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction permettant de créer les bases d'apprentissage et de test\n",
    "# params :\n",
    "#    dataframe : dataframe contenant les données\n",
    "#    column_target : string de la colonne-cible à estimer\n",
    "#    test_size : ratio de la base de test (30% par défaut)\n",
    "# returns :\n",
    "#    base : tuple des bases d'apprentissage et de test\n",
    "def split_base(dataframe, column_target, test_size = 0.3):\n",
    "    \n",
    "    # Gestion erreur test_size\n",
    "    if(test_size <= 0 or test_size >= 1):\n",
    "        return \"Erreur sur le ratio de test_size : vérifier que test_size est compris entre 0 et 1. \\n\"\n",
    "    \n",
    "    # Gestion erreur column_target comme string\n",
    "    elif (isinstance(column_target, str) == False):\n",
    "        return \"Erreur sur le type de column_target : vérifier que column_target est bien une string. \\n\"\n",
    "    \n",
    "    else:\n",
    "        train, test = train_test_split(dataframe, test_size = test_size)\n",
    "\n",
    "        X_train = train.drop(column_target, axis=1)\n",
    "        y_train = train[column_target]\n",
    "\n",
    "        X_test = test.drop(column_target, axis=1)\n",
    "        y_test = test[column_target]\n",
    "        \n",
    "        base = X_train, y_train, X_test, y_test\n",
    "        \n",
    "        return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_on_pipeline(pipeline, bool_type_modele, base):\n",
    "    '''\n",
    "        Permet d'appeler les bons indicateurs de performances et de récupérer\n",
    "        les informations utiles selon la pipeline passé en paramètre. Retourne les\n",
    "        performances associées.\n",
    "        \n",
    "        :params:\n",
    "            pipeline : sortie de la pipeline\n",
    "            bool_type_modele : booléen pour le type du modèle (0: régression / 1: classification)\n",
    "            base : tuple des bases d'apprentissage et de test\n",
    "        \n",
    "        :type params:\n",
    "            bool_type_modele: boolean\n",
    "            base: tuple\n",
    "        \n",
    "        :return: les performances des différents indicateurs et graphiques\n",
    "    '''\n",
    "    if (bool_type_modele == 0) : \n",
    "        print(\"Choix du type d'estimateur : Régression \\n\")\n",
    "        perfs = indic_perform_reg(pipeline, base)\n",
    "        \n",
    "        # poster les scores dans la database \n",
    "    \n",
    "    else:\n",
    "        print(\"Choix du type d'estimateur : Classification \\n\")\n",
    "        perfs = indic_perform_class(pipeline, base)\n",
    "        \n",
    "        # poster les scores dans la database\n",
    "        \n",
    "    return perfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction permettant d'obtenir les résultats des indicateurs de performances d'une régression\n",
    "# params :\n",
    "#    pipeline : sortie de la pipeline\n",
    "#    base : tuple des bases d'apprentissage et de test\n",
    "# returns :\n",
    "#    rmse : score RMSE\n",
    "#    r2 : score R²\n",
    "#    cross_val : score de cross-validation\n",
    "def indic_perform_reg(pipeline, base):\n",
    "    ''' # Fonction permettant d'obtenir les résultats des indicateurs de performances d'une régression\n",
    "        # params :\n",
    "        #    pipeline : sortie de la pipeline\n",
    "        #    base : tuple des bases d'apprentissage et de test\n",
    "    '''\n",
    "    # Pré-process ?\n",
    "    pipeline_fit = pipeline.fit(base[0],base[1]) # besoin du fit ou seulement du préd ? \n",
    "    pred = pipeline_fit.predict(base[2]) # besoin du predict ou déjà fait ? \n",
    "    \n",
    "    # Calcul du RMSE\n",
    "    rmse = sqrt(mean_squared_error(base[3], pred))\n",
    "    \n",
    "    # Calcul du R²\n",
    "    r2 = r2_score(base[3], pred)\n",
    "    \n",
    "    # Cross-Validation\n",
    "    cross_val = np.mean(cross_val_score(pipeline, base[2], base[3], cv=5))\n",
    "    \n",
    "    return rmse, r2, cross_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction permettant d'obtenir les résultats des indicateurs de performances d'une classification\n",
    "# params :\n",
    "#    pipeline : sortie de la pipeline\n",
    "#    X_test : variables de test pour évaluer les performances du modèle\n",
    "#    y_test : valeurs de test pour évaluer les performances du modèle\n",
    "# returns :\n",
    "#    matrice_confu : matrice de confusion\n",
    "#    classif_report : classification report\n",
    "def indic_perform_class(pipeline, X_test, y_test):\n",
    "    # Pré-process ?\n",
    "    pipeline_fit = pipeline.fit(X_train,y_train) # besoin du fit ou seulement du préd ? \n",
    "    pred = pipeline_fit.predict(X_test) # besoin du predict ou déjà fait ? \n",
    "    \n",
    "    # Calcul Matrice de confusion\n",
    "    matrice_confu = False\n",
    "    \n",
    "    # Calcul du Classification Report\n",
    "    classif_report = False\n",
    "    \n",
    "    return matrice_confu, classif_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading du dataframe et description\n",
    "df_test = load_all_data(\"./../data/headbrain.csv\")\n",
    "#descript_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choix du type d'estimateur : Régression \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(78.62805196337727, 0.566006456290973, 0.472519474482436)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nettoyage en amont / Choix des colonnes à utiliser ???\n",
    "\n",
    "# Splitting entre apprentissage et test\n",
    "base = split_base(df_test, \"Brain Weight(grams)\")\n",
    "\n",
    "# Pipeline à utiliser\n",
    "pipe_test = Pipeline([\n",
    "    ('features', StandardScaler()),\n",
    "    ('estimator', neighbors.KNeighborsRegressor())   \n",
    "])\n",
    "\n",
    "# Calculer les performances selon le type d'estimateur\n",
    "perform = perform_on_pipeline(pipe_test, 0, base)\n",
    "perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function perform_on_pipeline in module __main__:\n",
      "\n",
      "perform_on_pipeline(pipeline, bool_type_modele, base)\n",
      "    Permet d'appeler les bons indicateurs de performances et de récupérer\n",
      "    les informations utiles selon la pipeline passé en paramètre. Retourne les\n",
      "    performances associées.\n",
      "    \n",
      "    :param a: sortie de la pipeline\n",
      "    :param b: booléen pour le type du modèle (0: régression / 1: classification)\n",
      "    :param c: tuple des bases d'apprentissage et de test\n",
      "    \n",
      "    :type b: boolean\n",
      "    :type c: tuple\n",
      "    \n",
      "    :return: les performances des différents indicateurs et graphiques\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(perform_on_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
